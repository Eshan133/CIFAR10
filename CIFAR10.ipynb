{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNz64iY3FlAoi4dSE83h+oB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eshan133/CIFAR10/blob/main/CIFAR10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Importing Packages"
      ],
      "metadata": {
        "id": "NhVY-rrUXqwy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuhjY9oPR6Pd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "XdRgQZvYX4EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "# 2. Importing the dataset **(CIFAR10)**"
      ],
      "metadata": {
        "id": "pRUiHkyVYD5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = datasets.CIFAR10(root=\"data\",\n",
        "                              train=True,\n",
        "                              download=True,\n",
        "                              transform=ToTensor(),\n",
        "                              target_transform=None)"
      ],
      "metadata": {
        "id": "CppnA9SfS0iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = datasets.CIFAR10(root=\"data\",\n",
        "                              train=False,\n",
        "                              download=True,\n",
        "                              transform=ToTensor(),\n",
        "                              target_transform=None)"
      ],
      "metadata": {
        "id": "CERmJbt6TWbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 3. Dataset Visualization"
      ],
      "metadata": {
        "id": "EX28AOx0YNVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data"
      ],
      "metadata": {
        "id": "8QBjWvjnTcvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length of train dataset: {len(train_data)}\")\n",
        "print(f\"Length of test dataset: {len(test_data)}\")"
      ],
      "metadata": {
        "id": "4h29Xb7OTejp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, label = train_data[0]\n",
        "class_names = train_data.classes\n",
        "\n",
        "print(f\"Classes present in the dataset: {class_names}\")"
      ],
      "metadata": {
        "id": "LC1b4bqgTsEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Visualizing a image from the dataset"
      ],
      "metadata": {
        "id": "niJ-daJIZD6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(img.permute(1,2,0).numpy())\n",
        "plt.title(class_names[label])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F_2P601KTwpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Visualizing and comparing **[color channel, height, weight]**"
      ],
      "metadata": {
        "id": "iSq4lcRGZNeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Image Shape: {img.shape} --> [color channel, height, weight]\")"
      ],
      "metadata": {
        "id": "rhaXj56jU6lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "color_channel = []\n",
        "height = []\n",
        "width = []\n",
        "\n",
        "for img, _ in train_data:\n",
        "  c, h, w = img.shape\n",
        "  color_channel.append(c)\n",
        "  height.append(h)\n",
        "  width.append(w)"
      ],
      "metadata": {
        "id": "8oBnx5_JVhIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "yiF9nlZRWJpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "color_channel = np.array(color_channel)\n",
        "height = np.array(height)\n",
        "width = np.array(width)"
      ],
      "metadata": {
        "id": "oeQ5dsm3W5T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1, 3, figsize=(18,6))\n",
        "\n",
        "axs[0].hist(height, color=\"blue\", bins=30)\n",
        "axs[0].set_title(\"Height Comparison\")\n",
        "axs[0].set_ylabel(\"Frequency\")\n",
        "axs[0].set_xlabel(\"Height\")\n",
        "\n",
        "axs[1].hist(height, color=\"green\", bins=30)\n",
        "axs[1].set_title(\"Width Comparison\")\n",
        "axs[1].set_ylabel(\"Frequency\")\n",
        "axs[1].set_xlabel(\"Width\")\n",
        "\n",
        "axs[2].hist(color_channel, color=\"red\", bins=30)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8fT1q3HIWBKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Visualizing the images present in the dataset"
      ],
      "metadata": {
        "id": "JCzDPxDAZvoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(9,9))\n",
        "rows = 3\n",
        "cols = 3\n",
        "\n",
        "for i in range(1, rows*cols+1):\n",
        "  rand_idx = torch.randint(1, len(train_data), size=[1]).item()\n",
        "  img, label = train_data[rand_idx]\n",
        "  fig.add_subplot(rows, cols, i)\n",
        "  plt.imshow(img.permute(1,2,0).numpy())\n",
        "  plt.title(class_names[label])\n",
        "  plt.axis(False)"
      ],
      "metadata": {
        "id": "3yyXg8ItWo3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 4. Preparing the DataLoader"
      ],
      "metadata": {
        "id": "XX1s-GgfbeqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "rBur5Db7bhza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Visualizing the DataLoader"
      ],
      "metadata": {
        "id": "AVbGsPG-b_T0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader, test_dataloader"
      ],
      "metadata": {
        "id": "o223_NONb-lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length of train data: {len(train_data)}\")\n",
        "print(f\"Length of test data: {len(test_data)}\")\n",
        "print('--------------------------------------------------------')\n",
        "print(f\"Length of train dataloader: {len(train_dataloader)} of size {BATCH_SIZE}...\")\n",
        "print(f\"Length of test dataloader: {len(test_dataloader)} of size {BATCH_SIZE}...\")"
      ],
      "metadata": {
        "id": "C-5P9Td2cEbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Extracting data from the dataloader **(CHECKING)**"
      ],
      "metadata": {
        "id": "-HWC9CKMdEq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For train dataloader\n",
        "train_feature, train_labels = next(iter(train_dataloader))"
      ],
      "metadata": {
        "id": "_LQCOAPrc_T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"FOR EACH ITERATION\")\n",
        "print(\"-------------------------------\")\n",
        "print(f\"Shape of train features [INPUT]: {train_feature.shape}\")\n",
        "print(f\"Shape of train labels [OUTPUT]: {train_labels.shape}\")"
      ],
      "metadata": {
        "id": "CnTapUnieQxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First data from one iteration of train_dataloader\n",
        "plt.imshow(train_feature[0].permute(1,2,0))\n",
        "plt.title(class_names[train_labels[0]])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G1zUncYjeRq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 5. Training and Testing loop"
      ],
      "metadata": {
        "id": "_FPV70zydEdA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 training_loop()"
      ],
      "metadata": {
        "id": "8DplUweIgZtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(model:torch.nn.Module,\n",
        "                  dataloader:torch.utils.data.DataLoader,\n",
        "                  loss_fn:torch.nn.Module,\n",
        "                  optimizer:torch.optim.Optimizer,\n",
        "                  accuracy_fn,\n",
        "                  device:torch.device=device):\n",
        "\n",
        "  model.train()\n",
        "  model.to(device)\n",
        "  train_loss, train_acc = 0, 0\n",
        "\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    # Froward Pass\n",
        "    y_pred = model(X)\n",
        "\n",
        "    # Calculate Loss\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss\n",
        "\n",
        "    # Calculate Accuracy\n",
        "    acc = accuracy_fn(y_pred=y_pred.argmax(dim=1), y_true=y)\n",
        "    train_acc += acc\n",
        "\n",
        "    # Optimizer Zero Grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Loss Backward\n",
        "    loss.backward()\n",
        "\n",
        "    # Optimizer Step\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print the no. of Batch completed\n",
        "    if(batch % 200 == 0):\n",
        "      print(f\"{batch} BATCH HAS BEEN COMPLETED..\")\n",
        "\n",
        "  train_acc /= len(dataloader)\n",
        "  train_loss /= len(dataloader)\n",
        "\n",
        "  print(f\"Train Loss: {train_loss:.5f} | Train Accuracy: {train_acc:.2f}%\")\n",
        "\n",
        "  return train_acc, train_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "2KHDYugOdH6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 testing_loop()"
      ],
      "metadata": {
        "id": "Au9DZTWUgiWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def testing_loop(model:torch.nn.Module,\n",
        "                 dataloader:torch.utils.data.DataLoader,\n",
        "                 loss_fn:torch.nn.Module,\n",
        "                 accuracy_fn,\n",
        "                 device:torch.device=device):\n",
        "\n",
        "  model.eval()\n",
        "  model.to(device)\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    test_acc, test_loss = 0, 0\n",
        "\n",
        "    for X,y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      y_pred = model(X)\n",
        "\n",
        "      loss = loss_fn(y_pred, y)\n",
        "      test_loss += loss\n",
        "\n",
        "      acc = accuracy_fn(y_pred=y_pred.argmax(dim=1), y_true=y)\n",
        "      test_acc += acc\n",
        "\n",
        "    test_acc /= len(dataloader)\n",
        "    test_loss /= len(dataloader)\n",
        "\n",
        "    print(f\"Test Loss: {test_loss:.5f} || Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    return test_acc, test_loss\n"
      ],
      "metadata": {
        "id": "dKDhIR4nf0aM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 6. Evaluation & Prediction Function"
      ],
      "metadata": {
        "id": "fUAKNHAAvWcE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 evaluation()"
      ],
      "metadata": {
        "id": "vG9sH37SvjyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(model:torch.nn.Module,\n",
        "               train_acc,\n",
        "                train_loss,\n",
        "                test_acc,\n",
        "                test_loss,\n",
        "                epochs:int,\n",
        "                learning_rate:float,\n",
        "                optimizer_name,\n",
        "                time_taken,\n",
        "                device:torch.device = device):\n",
        "\n",
        "  return {\n",
        "      'model_name': model.__class__.__name__,\n",
        "      'learning_rate':learning_rate,\n",
        "      'epochs':epochs,\n",
        "      'optimizer':optimizer_name,\n",
        "      'train_loss': train_loss.item(),\n",
        "      'train_accuracy':train_acc,\n",
        "      'test_loss': test_loss.item(),\n",
        "      'test_accuracy':test_acc,\n",
        "      'time_taken':time_taken,\n",
        "      'device':device\n",
        "  }"
      ],
      "metadata": {
        "id": "NWS8xLZgveO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 plot_metrics()"
      ],
      "metadata": {
        "id": "fFkpMWo8CQcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metrics(train_losses, test_losses, train_accuracies, test_accuracies):\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "    # Create subplots\n",
        "    fig, ax1 = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    # Plot Loss\n",
        "    ax1[0].plot(epochs, train_losses, label=\"Train Loss\", marker='o')\n",
        "    ax1[0].plot(epochs, test_losses, label=\"Test Loss\", marker='o')\n",
        "    ax1[0].set_xlabel(\"Epochs\")\n",
        "    ax1[0].set_ylabel(\"Loss\")\n",
        "    ax1[0].set_title(\"Training and Testing Loss\")\n",
        "    ax1[0].legend()\n",
        "\n",
        "    # Plot Accuracy\n",
        "    ax1[1].plot(epochs, train_accuracies, label=\"Train Accuracy\", marker='o')\n",
        "    ax1[1].plot(epochs, test_accuracies, label=\"Test Accuracy\", marker='o')\n",
        "    ax1[1].set_xlabel(\"Epochs\")\n",
        "    ax1[1].set_ylabel(\"Accuracy\")\n",
        "    ax1[1].set_title(\"Training and Testing Accuracy\")\n",
        "    ax1[1].legend()\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "cqB2m235CXII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 make_predictions()"
      ],
      "metadata": {
        "id": "q-CratH2vtJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions(model:torch.nn.Module,\n",
        "                     data:list,\n",
        "                     device:torch.device=device):\n",
        "\n",
        "  pred_probs = []\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    for sample in data:\n",
        "\n",
        "      sample = torch.unsqueeze(sample, dim=0).to(device)\n",
        "\n",
        "      pred_logits = model(sample)\n",
        "\n",
        "      pred_prob = torch.softmax(pred_logits.squeeze(), dim=0)\n",
        "\n",
        "      pred_probs.append(pred_prob.cpu())\n",
        "\n",
        "  return torch.stack(pred_probs)"
      ],
      "metadata": {
        "id": "34wz4W7ovhUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 7. Setting up Loss & Accuracy Function"
      ],
      "metadata": {
        "id": "e9ThnMOhoEHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Download helper function\n",
        "if Path(\"helper_function.py\").is_file():\n",
        "  print(\"helper_function.py already exists, skipping download>>\")\n",
        "else:\n",
        "  print(\"Downloading helper_function.py>>>>>>>>\")\n",
        "  request = requests.get(\"https://raw.githubusercontent.com/Eshan133/pytorch-deep-learning/refs/heads/main/helper_functions.py\")\n",
        "  with open(\"helper_function.py\", \"wb\") as f:\n",
        "    f.write(request.content)\n",
        "    print(\"Downloading complete\")"
      ],
      "metadata": {
        "id": "Owp2ZuiLoJJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the accuracy metircs from helper_function.py\n",
        "from helper_function import accuracy_fn\n",
        "\n",
        "#setup loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "At23YFbcoNzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 8. Timer Function"
      ],
      "metadata": {
        "id": "JVAlbZcewJFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "def print_time(start:float, end:float, device:torch.device=None):\n",
        "  total_time = end - start\n",
        "  print(f\"Total time taken on {device}: {total_time:.3f} seconds\")\n",
        "  return total_time"
      ],
      "metadata": {
        "id": "bBVHMI9IwNAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 9. Base Model"
      ],
      "metadata": {
        "id": "WQBI6CvYiVjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tiny VGG net\n",
        "class ModelV1(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_units:int,\n",
        "               hidden_units:int,\n",
        "               output_units:int):\n",
        "    super().__init__()\n",
        "\n",
        "    ## Feature Extraction Layers\n",
        "    self.conv_block_1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=input_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  padding=1,\n",
        "                  stride=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  padding=1,\n",
        "                  stride=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "\n",
        "    self.conv_block_2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  padding=1,\n",
        "                  stride=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  padding=1,\n",
        "                  stride=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "\n",
        "    ## Classifier/Output layer\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=hidden_units*8*8,\n",
        "                  out_features=output_units),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv_block_1(x)\n",
        "    # print(x.shape)\n",
        "\n",
        "    x = self.conv_block_2(x)\n",
        "    # print(x.shape)\n",
        "\n",
        "    x = self.classifier(x)\n",
        "    # print(x.shape)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "jCvJVp-7iS6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.1 Initializing the base model"
      ],
      "metadata": {
        "id": "d8yltBOMlI_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelV1 = ModelV1(input_units=3,\n",
        "                  hidden_units=10,\n",
        "                  output_units=len(class_names)).to(device)"
      ],
      "metadata": {
        "id": "ODiBqWqJlMU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.2 Setting up Optimizer of base model"
      ],
      "metadata": {
        "id": "KQzFlcGAnz7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.1\n",
        "optimizer = torch.optim.SGD(params=modelV1.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "KM-060BKn38l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "## 9.3 Finding the shape of Linear Layer\n"
      ],
      "metadata": {
        "id": "XBwoDkTSmsuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we need to keep one thing in mind:\n",
        "- Linear Layer, takes input shape that doesn't match the hidden unit. `i.e` the output of `conv_block_2`.\n",
        "- Hence, we need to create a dummy data of same shape as the input, and print the shape after each layer to identify the correct shape.\n",
        "- Let's do that now....\n",
        "  - Shape of output of `conv_block_2`: torch.Size([1, 10, 8, 8])\n",
        "  - So, let's multiply the `in_features` by $8*8$"
      ],
      "metadata": {
        "id": "_1qt0lTwkans"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_data = torch.rand(size = (3, 32, 32))\n",
        "\n",
        "modelV1(dummy_data.unsqueeze(0).to(device))"
      ],
      "metadata": {
        "id": "3QWksiBck4M1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "## 9.4 Training the Base Model"
      ],
      "metadata": {
        "id": "6QIlBVUXnDAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "tU1ob_4AnLvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "\n",
        "start_time_1 = timer()\n",
        "train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n",
        "train_losses, test_losses = [], []\n",
        "train_accuracies, test_accuracies = [], []\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"--- Epoch: {epoch} ---\")\n",
        "  train_acc, train_loss = training_loop(model=modelV1,\n",
        "                                        dataloader=train_dataloader,\n",
        "                                        loss_fn=loss_fn,\n",
        "                                        accuracy_fn=accuracy_fn,\n",
        "                                        optimizer=optimizer,\n",
        "                                        device=device)\n",
        "\n",
        "  test_acc, test_loss = testing_loop(model=modelV1,\n",
        "                                      dataloader=test_dataloader,\n",
        "                                      loss_fn=loss_fn,\n",
        "                                      accuracy_fn=accuracy_fn,\n",
        "                                      device=device)\n",
        "  print(\"-------------------------------\")\n",
        "\n",
        "  # Store metrics\n",
        "  train_losses.append(train_loss.item())\n",
        "  test_losses.append(test_loss.item())\n",
        "  train_accuracies.append(train_acc)\n",
        "  test_accuracies.append(test_acc)\n",
        "\n",
        "end_timer_1 = timer()\n",
        "\n",
        "# Plot the metrics\n",
        "plot_metrics(train_losses, test_losses, train_accuracies, test_accuracies)\n",
        "\n",
        "total_time_model_V1 = print_time(start=start_time_1, end=end_timer_1, device=device)"
      ],
      "metadata": {
        "id": "Itmaz73ylxdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.5 Evaluating the Base Model"
      ],
      "metadata": {
        "id": "yBN7ePq1wrXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model_V1 = evaluation(model=modelV1,\n",
        "                           train_acc=train_acc,\n",
        "                           train_loss=train_loss,\n",
        "                           test_acc=test_acc,\n",
        "                           test_loss=test_loss,\n",
        "                            epochs=epochs,\n",
        "                            learning_rate=learning_rate,\n",
        "                            optimizer_name='SGD',\n",
        "                            time_taken=total_time_model_V1,\n",
        "                            device=device)"
      ],
      "metadata": {
        "id": "G8vislNCpIPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model_V1"
      ],
      "metadata": {
        "id": "4alLQpW0wyjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 10. Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "yvJBRM-I0_M_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.1 Increasing epochs"
      ],
      "metadata": {
        "id": "kzdJ1hZ5_mjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelV1_1 = ModelV1(input_units=3,\n",
        "                  hidden_units=10,\n",
        "                  output_units=len(class_names)).to(device)"
      ],
      "metadata": {
        "id": "g5FusvJ51Y6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.1\n",
        "optimizer = torch.optim.SGD(params=modelV1_1.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "jgH5snqI1fKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "\n",
        "start_time_1 = timer()\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"--- Epoch: {epoch} ---\")\n",
        "  train_acc, train_loss = training_loop(model=modelV1_1,\n",
        "                dataloader=train_dataloader,\n",
        "                loss_fn=loss_fn,\n",
        "                accuracy_fn=accuracy_fn,\n",
        "                optimizer=optimizer,\n",
        "                device=device)\n",
        "\n",
        "  test_acc, test_loss = testing_loop(model=modelV1_1,\n",
        "              dataloader=test_dataloader,\n",
        "              loss_fn=loss_fn,\n",
        "              accuracy_fn=accuracy_fn,\n",
        "              device=device)\n",
        "  print(\"-------------------------------\")\n",
        "\n",
        "  # Store metrics\n",
        "  train_losses.append(train_loss.item())\n",
        "  test_losses.append(test_loss.item())\n",
        "  train_accuracies.append(train_acc)\n",
        "  test_accuracies.append(test_acc)\n",
        "\n",
        "end_timer_1 = timer()\n",
        "\n",
        "# Plot the metrics\n",
        "plot_metrics(train_losses, test_losses, train_accuracies, test_accuracies)\n",
        "\n",
        "total_time_model_V1 = print_time(start=start_time_1, end=end_timer_1, device=device)"
      ],
      "metadata": {
        "id": "9NLKEC4mx_Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model_V1_1 = evaluation(model=modelV1_1,\n",
        "                             train_acc=train_acc,\n",
        "                              train_loss=train_loss,\n",
        "                              test_acc=test_acc,\n",
        "                              test_loss=test_loss,\n",
        "                              epochs=epochs,\n",
        "                              learning_rate=learning_rate,\n",
        "                              optimizer_name='SGD',\n",
        "                              time_taken=total_time_model_V1,\n",
        "                              device=device)"
      ],
      "metadata": {
        "id": "0JnTSMCS1puP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model_V1_1"
      ],
      "metadata": {
        "id": "Xib2OQpV3oHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train_loss, test_loss, comments"
      ],
      "metadata": {
        "id": "IM7ToY7634Aw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "ylGvH0mW9AqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comparision_df = pd.DataFrame([eval_model_V1,eval_model_V1_1])\n",
        "comparision_df['comment'] = ['Let\\'s try increasing the epoch',\n",
        "                             'Increasing epoch is still giving better results, Also the jump is crazy']"
      ],
      "metadata": {
        "id": "5l22lMdD38I_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comparision_df"
      ],
      "metadata": {
        "id": "BKAsUcsM9CCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----"
      ],
      "metadata": {
        "id": "6RXShuR7_Ksz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Epoch: 20\n",
        "\n",
        "modelV1_2 = ModelV1(input_units=3,\n",
        "                  hidden_units=10,\n",
        "                  output_units=len(class_names)).to(device)\n",
        "\n",
        "learning_rate = 0.1\n",
        "optimizer = torch.optim.SGD(params=modelV1_2.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "start_time_1 = timer()\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"--- Epoch: {epoch} ---\")\n",
        "  train_acc, train_loss = training_loop(model=modelV1_2,\n",
        "                dataloader=train_dataloader,\n",
        "                loss_fn=loss_fn,\n",
        "                accuracy_fn=accuracy_fn,\n",
        "                optimizer=optimizer,\n",
        "                device=device)\n",
        "\n",
        "  test_acc, test_loss = testing_loop(model=modelV1_2,\n",
        "              dataloader=test_dataloader,\n",
        "              loss_fn=loss_fn,\n",
        "              accuracy_fn=accuracy_fn,\n",
        "              device=device)\n",
        "  print(\"-------------------------------\")\n",
        "\n",
        "  # Store metrics\n",
        "  train_losses.append(train_loss.item())\n",
        "  test_losses.append(test_loss.item())\n",
        "  train_accuracies.append(train_acc)\n",
        "  test_accuracies.append(test_acc)\n",
        "\n",
        "\n",
        "end_timer_1 = timer()\n",
        "\n",
        "# Plot the metrics\n",
        "plot_metrics(train_losses, test_losses, train_accuracies, test_accuracies)\n",
        "\n",
        "total_time_model_V1 = print_time(start=start_time_1, end=end_timer_1, device=device)"
      ],
      "metadata": {
        "id": "I4gnA60T-41j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model_V1_2 = evaluation(model=modelV1_2,\n",
        "                             train_acc=train_acc,\n",
        "                              train_loss=train_loss,\n",
        "                              test_acc=test_acc,\n",
        "                              test_loss=test_loss,\n",
        "                              epochs=epochs,\n",
        "                              learning_rate=learning_rate,\n",
        "                              optimizer_name='SGD',\n",
        "                              time_taken=total_time_model_V1,\n",
        "                              device=device)"
      ],
      "metadata": {
        "id": "0KMHp0ke_Uai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comparision_df = pd.DataFrame([eval_model_V1,eval_model_V1_1, eval_model_V1_2])\n",
        "comparision_df['comment'] = ['Let\\'s try increasing the epoch',\n",
        "                             'Increasing epoch is still giving better results',\n",
        "                             'Shows sign of overfitting & jumps crazy']"
      ],
      "metadata": {
        "id": "ooobBQaI_jM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comparision_df"
      ],
      "metadata": {
        "id": "2nxWkIAEAwAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.2 Data Augmentation"
      ],
      "metadata": {
        "id": "LtXu_v6BEm5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define Data Augmentation for Training Set\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616])\n",
        "])\n",
        "\n",
        "\n",
        "# Test Transform (No Augmentation)\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616])\n",
        "])\n"
      ],
      "metadata": {
        "id": "Q0GKwtNoEuG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Load the dataset with augmentation for training set\n",
        "train_dataset = datasets.CIFAR10(root=\"data\", train=True, transform=train_transform, download=True)\n",
        "test_dataset = datasets.CIFAR10(root=\"data\", train=False, transform=test_transform, download=True)\n",
        "\n",
        "# Create DataLoaders\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
      ],
      "metadata": {
        "id": "4gJOgprlFtJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataloader)"
      ],
      "metadata": {
        "id": "yXIs197TGmMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Epoch: 20\n",
        "\n",
        "modelV1_3 = ModelV1(input_units=3,\n",
        "                  hidden_units=10,\n",
        "                  output_units=len(class_names)).to(device)\n",
        "\n",
        "learning_rate = 0.1\n",
        "optimizer = torch.optim.SGD(params=modelV1_3.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 9\n",
        "\n",
        "start_time_1 = timer()\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"--- Epoch: {epoch} ---\")\n",
        "  train_acc, train_loss = training_loop(model=modelV1_3,\n",
        "                dataloader=train_dataloader,\n",
        "                loss_fn=loss_fn,\n",
        "                accuracy_fn=accuracy_fn,\n",
        "                optimizer=optimizer,\n",
        "                device=device)\n",
        "\n",
        "  test_acc, test_loss = testing_loop(model=modelV1_3,\n",
        "              dataloader=test_dataloader,\n",
        "              loss_fn=loss_fn,\n",
        "              accuracy_fn=accuracy_fn,\n",
        "              device=device)\n",
        "  print(\"-------------------------------\")\n",
        "\n",
        "  # Store metrics\n",
        "  train_losses.append(train_loss.item())\n",
        "  test_losses.append(test_loss.item())\n",
        "  train_accuracies.append(train_acc)\n",
        "  test_accuracies.append(test_acc)\n",
        "\n",
        "\n",
        "end_timer_1 = timer()\n",
        "\n",
        "# Plot the metrics\n",
        "plot_metrics(train_losses, test_losses, train_accuracies, test_accuracies)\n",
        "\n",
        "total_time_model_V1 = print_time(start=start_time_1, end=end_timer_1, device=device)"
      ],
      "metadata": {
        "id": "VX7hMpkqGO5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model_V1_3 = evaluation(model=modelV1_3,\n",
        "                             train_acc=train_acc,\n",
        "                              train_loss=train_loss,\n",
        "                              test_acc=test_acc,\n",
        "                              test_loss=test_loss,\n",
        "                              epochs=epochs,\n",
        "                              learning_rate=learning_rate,\n",
        "                              optimizer_name='SGD',\n",
        "                              time_taken=total_time_model_V1,\n",
        "                              device=device)"
      ],
      "metadata": {
        "id": "yc5GZPblHXch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Different Architectures:**\n",
        "\n",
        "- **ResNet:** A deeper but efficient model that often generalizes well.\n",
        "- **MobileNet:** A lightweight model that works well on CIFAR-10 with good accuracy.\n",
        "- **EfficientNet:** Scales well and has strong generalization capabilities"
      ],
      "metadata": {
        "id": "uvG2FzTlA0fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VGG19\n",
        "\n",
        "class ModelV2(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_units,\n",
        "               hidden_units,\n",
        "               output_units):\n",
        "     super().__init__()\n",
        "\n",
        "     self.conv_block_1 = nn.Sequential(\n",
        "         nn.Conv2d(in_channels=input_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  padding=1,\n",
        "                  stride=1),\n",
        "         nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  padding=1,\n",
        "                  stride=1),\n",
        "\n",
        "     )\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3FC0dNUTA0Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "btrsU-PYAyFq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}